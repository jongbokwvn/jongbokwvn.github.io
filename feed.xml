<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://wvnvwn.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://wvnvwn.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-03T12:30:59+00:00</updated><id>https://wvnvwn.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">[review] Training Language Models to Follow Instructions with Human Feedback</title><link href="https://wvnvwn.github.io/blog/2025/distill/" rel="alternate" type="text/html" title="[review] Training Language Models to Follow Instructions with Human Feedback"/><published>2025-05-03T00:00:00+00:00</published><updated>2025-05-03T00:00:00+00:00</updated><id>https://wvnvwn.github.io/blog/2025/%08distill</id><content type="html" xml:base="https://wvnvwn.github.io/blog/2025/distill/"><![CDATA[<h2 id="abstract">Abstract</h2> <h3 id="main-question">Main Question</h3> <p><strong>대규모 언어모델을 사용자의 지시(instruction)에 맞추어 안전하고 유용하게 동작하도록 정렬(alignment)하려면 어떤 학습 절차가 효과적인가?</strong></p> <ul> <li>모델 크기 증가에 따른 성능 증가 (scaling law)가 사용자의 의도를 더 잘 반영하지만은 않는다.</li> <li>사용자의 의도와 목적에 맞는 모델 출력을 위해서는 human feedback이 반영된 데이터셋 기반의 미세 조정이 필요하다.</li> </ul> <p> ### Suggestion **3단계 RLHF 기반 학습 파이프라인 제안** - OpenAI API를 통해 제출된 사용자 프롬프트와 labeler들이 작성한 프롬프트를 수집하고, 각 프롬프트에 대해 여러 개의 모델 응답을 생성한 뒤, 생성된 응답에 대해 평가자들이 적절한 응답 순위를 작성 - 수집된 데이터셋을 활용하여 강화학습 기법 통해 GPT-3 모델을 지도학습 방식으로 fine-tuning 진행 <p> ### Contribution - 100배 더 적은 파라미터를 가진 InstructGPT가 더 적합한 답변을 생성하며, 더 적은 독성 답변 생성과 신뢰가능한 답변 생성을 해내는 것으로 확인됨 - human feedback을 활용한 fine-tuning을 통해 언어 모델을 사용자의 의도에 정렬할 수 있음을 확인 --- ## Motivation Language modeling의 목적식은 next token prediction과 같은 방식의 최적화를 추구한다는 점에서, 사용자의 instruction을 따르는 최적화 목적식과는 다르기 때문 (The language modeling objective used for many recent large LMs is different from the objective ‘follow the user’s instructions helpfully and safely’. We say that the language modeling objective is misaligned.) - 강화학습의 알고리즘을 활용하여, human preference를 reward signal로 사용하여 모델을 fine-tuning하여 user preference 혹은 user intent를 최적화 목적식으로 설정 --- ## Data Collection 어떤 데이터를 학습에 사용했는가? <p> ### Prompts API를 통해 실제 사용자가 제출한 입력 prompt와 labeler-written instruction-like prompt를 사용 - 실제 사용자 prompt - Playground 인터페이스틑 롱해 InstructGPT 초기 모델을 사용한 사용자들이 제출한 프롬프트만 수집 (실제 API 사용자들의 데이터는 사용하지 않음) - 프롬프트 전처리 - 유사 문장의 반복을 막기 위해 공통적인 prefix 부분을 제거 - 유저당 최대 200개의 prompt만을 선택 - train/valid/test split을 진행 - 개인정보 필터링: PII 삭제 <p> - labeler-written instruction-like prompt - 초기 모델 학습에 필요한 instruction-following 데이터 확보를 위해 labeler-written prompt를 사용 - bootstrap 역할 수행 - Plain, Few-shot, User-based 종류로 prompt를 작성 <p> <d-figure> <img src="/assets/img/2025-05-03-distill-1.png" alt="선호도 증가" width="500"/> <figcaption>Figure 1: API Prompt Composition</figcaption> </d-figure><p> ### 데이터셋 구성 - SFT 데이터셋: Prompt + Labeler-written demonstrations - RM 데이터셋: Prompt + 모델 출력에 대한 pair-wise ranking - PPO 데이터셋: Prompt와 reward 값 (사실상 prompt만 사용하고 prompt에 대한 답변의 reward만 사용) --- ## Training Methodology 어떤 학습 방법을 사용했는가? <p> 1. **Supervised Fine-tuning** - 40명의 라벨러들이 사용자 프롬프트에 대한 이상적인 응답을 작성 - 프롬프트-응답 쌍을 활용하여 모델을 fine-tuning <p> 2. **Reward Model 학습** - 어떤 응답이 더 우수한지를 예측하는 reward model을 훈련 - Reward Model - SFT 모델에서 **final unembedding layer**를 제거하고, MLP를 붙여 scalar output을 반환하도록 구성 - transformer layer를 통과한 hidden state가 MLP를 통과하여 scalar 값을 반환하는 구조로 reward model을 구성 - 학습 데이터 - 같은 prompt에 대해 생성된 서로 다른 두 응답에 대해, 어느 쪽이 더 나은지를 사람이 라벨링한 데이터를 학습 - 이때, 하나의 prompt에 대해 K = 4~9개의 응답을 수집하고, 주어진 K개의 응답에서 서로 다른 2개의 응답 비교쌍을 만들어서 활용 - 하나의 prompt에 대한 모든 비교 쌍을 하나의 batch element로 학습하여 correlated sample 문제 해결 (overfitting의 원인) - 손실 함수 - $\text{loss }(\theta) = -{1\over \binom{k}{2}}E_{(x, y_w, y_l) \sim D}[\log (\sigma(r_\theta(x, y_w) - r_\theta(x, y_l)))]$ <p> 3. **RLHF with PPO** - PPO 알고리즘을 적용하여 모델을 강화학습 방식으로 추가 fine-tuning - Bandit Environment - 개념: 단 한 번의 행동만으로 즉각적인 보상을 받고 episode가 종료되는 구조 - 주어진 prompt에 대한 response의 step-wise 최적화를 위해 bandit environment를 가정 - Value Model - 학습된 RM model을 초기값으로 하여 모델과 함께 업데이트 됨 - 손실 함수 (Policy Loss) - $\text{objective }(\phi) = E_{(x, y) \sim D_{\pi_\phi^{RL}}}[r_\theta(x, y) - \beta \log({\pi_\phi^{RL}(y | x) \over \pi^{\text{SFT}}(y|x)})] + \gamma E_{x \sim D_{\text{pretrain}}}[\log(\pi_\phi^{RL}(x))]$ - KL-divergence 항 - if $\gamma \neq 0$, PPO-ptx model --- ## Evaluation Metric 성능 평가를 어떻게 진행하였는가? <p> ### Baseline Model - PPO 모델을 GPT-3 &amp; GPT-3-prompted 모델 및 SFT 모델과 비교 <p> ### Alignment의 개념 - 사용자의 의도에 부합되기 훈련된 모델이 행동하는 것 - helpful: they should help the user solve their task - 모델은 instruction에 부합한 output을 생성해야 - honest: they shouldn’t fabricate information or mislead the user - hallucination을 유발하는 것인지를 평가하고 - TruthfulQA dataset을 활용하여 평가 - harmless: they shouldn’t cause physical, psychological, or social harm to people or the environment --- ## Experiments 1. **실사용 API에 대한 결과 - 175B 모델** - **API Prompt 결과** - GPT &lt; GPT-prompted &lt; SFT &lt; PPO &lt; PPO-ptx 순으로 선호도가 증가하는 경향을 보임. <d-figure> <img src="/assets/img/2025-05-03-distill-2.png" alt="선호도 증가"/> <figcaption>Figure 2: API Prompt 결과 선호도 비교</figcaption> </d-figure> - 세부 기준에 따라 선호도를 평가한 결과. <d-figure> <img src="/assets/img/2025-05-03-distill-3.png" alt="선호도 증가"/> <figcaption>Figure 3: 세부 기준 선호도 평가 결과</figcaption> </d-figure> - **InstructGPT의 일반화 성능** - Reward model의 학습 과정 참여 여부를 기준으로 worker들을 구분하여, InstructGPT가 reward model에 과도하게 optimize 되었는지를 분석하고, 훈련 과정에 사용된 InstructGPT prompt와 그렇지 않은 GPT-3 prompt를 사용하여 선호로들 평가함으로써 InstructGPT의 일반화 성능을 평가. <d-figure> <img src="/assets/img/2025-05-03-distill-4.png" alt="선호도 증가"/> <figcaption>Figure 4: InstructGPT 일반화 성능 평가 결과</figcaption> </d-figure> - **공개된 NLP dataset-based FT 모델과 비교** - FLAN과 T0 dataset으로 FT 모델과 비교한 결과, SFT 모델보다는 좋지 못함 ⇒ 이 데이터셋은 선호도 반영을 위해서 언어 모델이 사용하는 방식을 반영하지 않음을 보임. <d-figure> <img src="/assets/img/2025-05-03-distill-5.png" alt="선호도 증가"/> <figcaption>Figure 5: 공개 NLP 데이터셋 기반 FT 모델 비교 결과</figcaption> </d-figure> 2. **공개 NLP Benchmark에 대한 결과** - **Truthfulness (Honesty)** <d-figure> <img src="/assets/img/2025-05-03-distill-6.png" alt="선호도 증가"/> <figcaption>Figure 6: Truthfulness (Honesty) 평가 결과</figcaption> </d-figure> - TruthfulQA dataset 평가 결과로, 각각 1.3/6/175B 모델 - 회색은 informative - human evaluation을 진행한 것<p><p> - **Toxicity** <d-figure> <img src="/assets/img/2025-05-03-distill-7.png" alt="선호도 증가"/> <figcaption>Figure 7: Toxicity 평가 결과</figcaption> </d-figure> - RealToxicityPrompts dataset 평가 결과 - human evaluation과 PerspectiveAPI 결과를 동시에 활용 - Respectful: safe and respectful output을 반환하도록 하는 prompt를 추가한 것<p><p> - **Alignment Tax** - **Alignment tax**: API 분포로 PPO 미세조정을 한 모델은 일반 NLP 벤치마크 성능이 저해되는 문제가 발생 - Alignment tax 문제를 극복하기 위해 PPO-ptx를 활용하여, 사전학습 gradient를 일정 비율로 혼합하여 모델을 업데이트하는 방법을 사용<p><p> 3. **정성 평가** - non-english language 혹은 코드에 대한 질문에 대해서도 답변을 수행. --- ## Discussion 의미 있다고 생각되는 Discussion 내용만 추림. - **The cost of increasing model alignment is modest relative to pretraining!** - **Alignment techniques are need to reduce alignment-tax!** --- ## Insight 1. **언어모델로 하여금 instruction을 따르는 답변을 생성하도록 훈련하고 guidance 하는 것과, 선호를 표현하고 가치를 따르는 것은 다른 개념이라는 점에서, 이를 명확하게 구분하는 것이 필요함** <p> 2. **Preference optimization 과정에서 instruction-following과 prefenrece-reveal 혹은 following을 명확히 구분하는 것이 필요하며, 이를 위해서는 개념을 명확히 분리하는 것이 필요함** </p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p>]]></content><author><name>Long Ouyang*</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[Fine-tune the language model with PPO algorithm using human feedback.]]></summary></entry><entry><title type="html">[review] The First Law of Complexodynamics</title><link href="https://wvnvwn.github.io/blog/2025/distill/" rel="alternate" type="text/html" title="[review] The First Law of Complexodynamics"/><published>2025-04-30T00:00:00+00:00</published><updated>2025-04-30T00:00:00+00:00</updated><id>https://wvnvwn.github.io/blog/2025/%08distill</id><content type="html" xml:base="https://wvnvwn.github.io/blog/2025/distill/"><![CDATA[<h2 id="backgrounds">Backgrounds</h2> <ul> <li><strong>열역학 제 2법칙 (the second law of thermodynamics)</strong> <ul> <li>닫힌 계에서 엔트로피는 시간이 지남에 따라 maximum value를 달성할 때까지 증가하거나 일정하게 유지됨</li> <li>닫힌 계에서, 시스템은 시간이 지남에 따라 ‘더 많은 방법으로 존재할 수 있는 상태’로 자연스럽게 이동한다는 법칙 <ul> <li>시간의 흐름은 무질서의 증가를 동반하게 됨</li> </ul> </li> </ul> </li> <li><strong>엔트로피</strong> <ul> <li>시스템의 또 다른 성질 (another property of the system)</li> <li>시스템이 얼마나 무작위적이고 일반적이며 무질서한지를 수치화한 것</li> <li>$ \mathcal S = k_{\mathcal B}\ln \Omega $: 가능한 미시상태(microstates)의 수에 대한 로그</li> </ul> </li> <li><strong>Kolmogorov Complexity</strong> <ul> <li>시스템의 복잡성(Complexity)을 측정하는 방법</li> <li>주어진 객체를 생성하는 가장 짧은 프로그램의 길이</li> <li>$ \mathcal K(\mathbf x) = \min_\mathcal p {|p| : U(\mathcal p) = \mathbf x} $</li> </ul> </li> <li><strong>Sophistication</strong> <ul> <li>데이터를 생성하는 데 필요한 가장 단순한 구조적 모델의 복잡도 <ul> <li>데이터에 포함된 정보 중 구조적인 정보와 무작위적인 정보를 구분하기 위해 제안</li> </ul> </li> <li>‘복잡성’ 계산에 있어 구조(structure)와 무작위(randomness)를 구분하기 위한 개념으로, kolmogorov complexity는 이 둘을 구분하지 못함</li> <li><strong>equation</strong> \(\text{Soph }_\mathbf c (\mathbf x) = \min \{ \mathcal K(\mathcal S) \| \mathbf x \in \mathcal S \text{ and } \mathcal K(\mathcal S) + \mathcal K(\mathbf x \| \mathcal S) \leq \mathcal K(\mathbf x) + \mathbf c \}\) <ul> <li><strong>annotation</strong> <ul> <li>$ \mathbf x \in \mathcal S $: 유한집합 $ \mathcal S $에 속한 문자열 $ \mathbf x $</li> <li>$ \mathbf c $: 중요도 상수 (significance constant)</li> <li>$ \mathcal K(\mathcal S) $: 유한집합 $ \mathcal S $의 kolmogorov complexity (유한집합을 기술하는 가장 짧은 프로그램의 길이)</li> <li>$ \mathcal K(\mathbf x | \mathcal S) $: 유한집합의 정보를 알고 있을 때 $ \mathbf x $를 특정하기 위해 추가로 필요한 정보량</li> </ul> </li> <li>sophistication = $ \mathbf x $를 설명하는 효율적인 모델 $ \mathcal S $를 찾는 과정에서, $ \mathbf x $를 $ \mathcal S $를 통해 설명하는 것이 직접 $ \mathbf x $를 설명하는 것만큼 효율적인 모델 들 중에서 가장 단순한 모델의 복잡도</li> <li>kolmogorov complexity와 달리 $ \mathbf x $가 속한 집합 $ \mathcal S $의 complexity를 활용하여 $ \mathbf x $의 complexity를 계산</li> </ul> </li> </ul> </li> </ul> <hr/> <h2 id="questions--answers">Questions &amp; Answers</h2> <p><strong>Question:</strong></p> <p>왜 열량 $ \delta Q $을 순간온도 $ T $로 나눈 값을 적분한 것이, 시스템의 ‘무질서도’, ‘일반성’, ‘무작위성’이라는 엔트로피 개념을 대표할 수 있는가?</p> \[\triangle S = \int{\delta Q\over T}\] <ul> <li>$ \delta $: inexact differential. 경로함수의 미소량</li> <li>$ Q $: 시스템이 외부로부터 받는 열 에너지</li> <li>$ T $: 열이 전달되는 경로 상의 순간 온도 (state variable에 해당)</li> </ul> <p><strong>Answer:</strong></p> <p>에너지는 시스템 내에서 어떤 온도에서 주고받는지가 매우 중요</p> <p>열량을 온도로 정규화함으로서, 추가적으로 주어지는 열이 시스템이 미치는 효과를 상대적으로 측정 가능</p> <ul> <li>고온의 환경에서, 추가적으로 주어지는 열은 시스템의 무질서 증가를 유발하지만, 그 정도는 상대적으로 작은 반면,</li> <li>저온의 환경에서, 추가적으로 주어지는 열은 시스템의 무질서 증가를 유발하며, 그 정도는 상대적으로 클 것이다.</li> </ul> <p>즉, 열량을 온도로 정규화함으로써, 주어진 에너지가 미시상태를 얼마나 증가시키는가를 측정할 수 있게 되는 것</p> <ul> <li>$ {\delta Q \over T} $: 시스템의 미시적 복잡성 증대량을 표현하는 정규화된 양</li> </ul> <hr/> <p><strong>Question:</strong></p> <p>열역학(thermodynamics) 관점에서의 ‘엔트로피’와 통계역학(statistical mechanics) 관점에서의 ‘엔트로피’ 간의 관련성</p> <p><strong>Answer:</strong></p> <p>definition of boltzmann entropy</p> <ul> <li>엔트로피 = 가능한 미시상태 수의 로그에 비례하는 양</li> <li>$ S = k \ln \Omega $ <ul> <li>$ k $: boltzmann 상수</li> <li>$ \Omega $: 가능한 미시상태 수</li> </ul> </li> </ul> <p>열량이 시스템에 주입되면, 입자들이 가질 수 있는 에너지 조합은 지수적으로 증가</p> <ul> <li>에너지가 배분되는 방식 하나하나가 하나의 미시상태를 정의함</li> <li>결국 미시상태 수는 주어진 열량 하에서 시스템이 가질 수 있는 가능한 구성(configuration) 수를 의미하며, $ \Omega $가 크다는 것은 시스템이 많은 다양한 방식으로 현재 상태를 유지할 수 있음을 의미</li> </ul> <p>많은 미시상태를 가지는 시스템은 다양한 내부 구성이 가능하다는 점에서 시스템의 복잡성(complexity)으로 나타남</p> <hr/> <h2 id="abstract">Abstract</h2> <ul> <li><strong>main question</strong> <ul> <li>Why does ‘complexity’ or ‘interestingness’ of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically? <ul> <li>물리계에서 ‘복잡도’는 시간의 흐름과 함께 최대치를 달성하고 나서 감소하는 경향을 보이는 반면, 엔트로피는 단조적으로 증가하는 경향을 보이는데, 그 이유는 무엇인가?</li> </ul> </li> </ul> </li> <li><strong>main answer</strong> <ul> <li>엔트로피는 단순히 ‘더 일반적인 미시상태(micro-state)로의 경향’을 표현하는 공리(tautology)에 가까우며, 본질적인 질문은 ‘왜 초기 상태에서의 엔트로피는 낮은가?’에 해당</li> <li>물리계에서 관찰되는 시간의 흐름에 따른 구조적 복잡성을 설명할 수 있는 수학적 지표의 필요성 <ul> <li>Kolmogorov complexity에 resource-bounded를 적용한 ‘Complextropy’를 제안하고, 샘플러와 복원기 두 알고리즘 모두에 계산 시간 제한을 걸면, 그 지표가 바로 원하는 물리계에서의 구조적 복잡성을 재현할 것이라는 추측(conjecture)을 제시</li> </ul> </li> </ul> </li> </ul> <hr/> <h2 id="content">Content</h2> <p><strong>(main question)</strong></p> <p>Why does ‘complexity’ or ‘interestingness’ of physical systems seem to increase with time and then hit a maximum and decrease, in contrast to the entropy, which of course increases monotonically?</p> <ul> <li>물리계에서 ‘복잡도’는 시간의 흐름과 함께 최대치를 달성하고 나서 감소하는 경향을 보이는 반면, 엔트로피는 단조적으로 증가하는 경향을 보이는데, 그 이유는 무엇인가?</li> </ul> <p><strong>(sub-question - part 1)</strong></p> <p>열역학 제 2법칙: 고립된 시스템은 시간이 지남에 따라 가능한 미시상태 수가 많은 일반적인 상태(generic state), 즉 엔트로피가 높은 상태로 진화하게 되는데, 이건 ‘<em>대부분의 상태로 시스템이 진화한다</em>‘는 명제와 논리상으로 동일</p> <ul> <li>‘엔트로피는 왜 단조적으로 증가하는가?’에 대한 질문보다,</li> <li>‘<strong>왜 초기 상태에서의 엔트로피는 낮은가</strong>‘에 대해 질문이 필요 (하지만 미스터리한 영역에 해당하므로, 주어진 명제로 수용하고 있음)</li> </ul> <p><strong>(sub-question - part 2)</strong></p> <p>Sean의 주장에 따르면, 닫힌 물리계에서 엔트로피는 단조적으로 증가하더라도, 시스템의 ‘complexity’ 혹은 ‘interesting’은 단조적으로 증가하지만은 않는다고 주장. 왜 그런 것인가?</p> <p><img src="/assets/img/250430.png" alt="Complexity vs Entropy over time"/></p> <ul> <li>초기 상태: 가능한 미시상태의 수가 적다는 점에서 시스템의 복잡성은 0에 수렴</li> <li>후기 상태: 가능한 미시상태의 수가 증가함에 따라 시스템이 평형(equilibrium)을 달성한다는 점에서 시스템의 복잡성은 0에 수렴</li> <li>중간 상태: 가능한 미시상태의 수가 증가하는 과정에서 시스템 평형을 달성하기 위한 상호작용이 발생함에 따라 시스템의 복잡성은 증가</li> </ul> <p>하지만 여기서,</p> <ul> <li>‘complexity’란 무엇인가? 하는 질문은 제기됨. <ul> <li>Sophistication의 개념을 통해 complexity를 정의할 수 있을 것</li> </ul> </li> </ul> <ol> <li>Kolmogorov complexity의 개념을 활용하여 entropy를 설명할 수 있음 <ul> <li>deterministic system에서 system의 complexity는 시간에 대해서 로그적으로 증가하게 됨 <ul> <li>deterministic system에서 state는 initial condition과 time step t에 의존하기 때문</li> <li>하지만 entropy 증가를 설명하기에는 kolmogorov complexity가 너무 느리게 증가</li> </ul> </li> <li>probabilistic system 혹은 resource-bounded kolmogorov complexity 개념을 활용한다면, entropy의 선형적 증가를 설명할 수 있음</li> </ul> </li> <li>Kolmogorov complexity의 개념을 활용하여 complextropy에 대해 설명 (kolmogorov complexity와 구별되는 complexity의 개념) <ul> <li>Kolmogorov complexity는 복잡성(complextropy) 계산에 있어 구조적 복잡성 계산 혹은 무작위적 복잡성 계산을 구분하여 계산하는 것이 불가능</li> <li>Sophistication은 단순, 난수 두 극단에서는 작고, 그러지 않은 단계에서는 커지는 척도에 해당하지만,</li> <li>여전히 deterministic system과 probabilistic system에서 probability transition function이 주어진 경우, complextropy가 logarithmical하게 증가한다는 점에서 여전히 동역학적 complextropy를 설명하는 것이 제한됨.</li> </ul> </li> <li>Complextropy with computationl resource bounds <ul> <li>computational resource bound를 적용한 n-bits 문자열 $ \mathbf x $의 complextropy <ul> <li>$ n \log n $ 시간에 작동하는 가장 짧은 프로그램 길이를 complextropy로 정의하되, 그 프로그램은 $ \mathcal S $에서 균등 샘플(uniform sample)을 생성하고, Oracle이 있더라도 $ \mathbf x $를 재생성하는 데 $ \log_2|\mathcal S| - c $ 비트가 필요해야 한다. <ul> <li>$ n \log n $ 시간: 시간 자원 제약을 통한 효율성 조건</li> <li>샘플 조건: 프로그램이 $ \mathcal S $에서 균등 샘플을 생성 <ul> <li>$ \mathbf x $가 $ \mathcal S $에서 전형적(generic)으로 보이도록 강제</li> </ul> </li> <li>복원 조건: Oracle이 있더라도 $ \mathbf x $를 재생성하는데 적어도 $ \log_2|\mathcal S| $의 비트가 필요</li> </ul> </li> <li>이중 효율성 제약 부과 <ul> <li>샘플조건과 복원조건에 동일한 시간 제약을 통해 중간 단계(intermediate)에서의 복잡성을 설명 가능</li> </ul> </li> </ul> </li> <li>The First Law of Complexodynamics <ul> <li>효율성 제약 하에서 complexotropy는 초기에는 작고, 중간에는 커지며, 종기에는 다시 작아진다</li> <li>증명되지 않았지만, 추측(conjecture)을 통해 증명 가능</li> </ul> </li> </ul> </li> </ol> <hr/> <h2 id="insight">Insight</h2> <ul> <li>엔트로피와 복잡성을 구분 <ul> <li>complextropy를 통해 엔트로피와 구분되는 물리계에서의 복잡성 특이점을 포착하고 이를 수학적으로 모델링하려는 시도</li> </ul> </li> <li>자원 제약의 물리학적 해석 <ul> <li>복잡성을 계산 가능한 함수로 모델링함으로써 물리적 복잡성에 따른 제약을 계산</li> <li>엔트로피 = 정보 (가능한 미시상태의 수)</li> <li>복잡성 = 자원제한 정보</li> </ul> </li> </ul>]]></content><author><name>Scott Aaronson</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[About the complexity and entropy of the model based on Kolmogorov Complexity.]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://wvnvwn.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://wvnvwn.github.io/blog/2025/plotly</id><content type="html" xml:base="https://wvnvwn.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://wvnvwn.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://wvnvwn.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://wvnvwn.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[<p>May 14, 2024[[read-time]] min read We’re introducing a series of updates across the Gemini family of models, including the new 1.5 Flash, our lightweight model for speed and efficiency, and Project Astra, our vision for the future of AI assistants. In December, we launched our first natively multimodal model Gemini 1.0 in three sizes: Ultra, Pro and Nano. Just a few months later we released 1.5 Pro, with enhanced performance and a breakthrough long context window of 1 million tokens.Developers and enterprise customers have been putting 1.5 Pro to use in incredible ways and finding its long context window, multimodal reasoning capabilities and impressive overall performance incredibly useful.We know from user feedback that some applications need lower latency and a lower cost to serve. This inspired us to keep innovating, so today, we’re introducing Gemini 1.5 Flash: a model that’s lighter-weight than 1.5 Pro, and designed to be fast and efficient to serve at scale.Both 1.5 Pro and 1.5 Flash are available in public preview with a 1 million token context window in Google AI Studio and Vertex AI. And now, 1.5 Pro is also available with a 2 million token context window via waitlist to developers using the API and to Google Cloud customers.We’re also introducing updates across the Gemini family of models, announcing our next generation of open models, Gemma 2, and sharing progress on the future of AI assistants, with Project Astra.Context lengths of leading foundation models compared with Gemini 1.5’s 2 million token capability1.5 Flash is the newest addition to the Gemini model family and the fastest Gemini model served in the API. It’s optimized for high-volume, high-frequency tasks at scale, is more cost-efficient to serve and features our breakthrough long context window.While it’s a lighter weight model than 1.5 Pro, it’s highly capable of multimodal reasoning across vast amounts of information and delivers impressive quality for its size.The new Gemini 1.5 Flash model is optimized for speed and efficiency, is highly capable of multimodal reasoning and features our breakthrough long context window.1.5 Flash excels at summarization, chat applications, image and video captioning, data extraction from long documents and tables, and more. This is because it’s been trained by 1.5 Pro through a process called “distillation,” where the most essential knowledge and skills from a larger model are transferred to a smaller, more efficient model.Read more about 1.5 Flash in our updated Gemini 1.5 technical report, on the Gemini technology page, and learn about 1.5 Flash’s availability and pricing.Over the last few months, we’ve significantly improved 1.5 Pro, our best model for general performance across a wide range of tasks.Beyond extending its context window to 2 million tokens, we’ve enhanced its code generation, logical reasoning and planning, multi-turn conversation, and audio and image understanding through data and algorithmic advances. We see strong improvements on public and internal benchmarks for each of these tasks.1.5 Pro can now follow increasingly complex and nuanced instructions, including ones that specify product-level behavior involving role, format and style. We’ve improved control over the model’s responses for specific use cases, like crafting the persona and response style of a chat agent or automating workflows through multiple function calls. And we’ve enabled users to steer model behavior by setting system instructions.We added audio understanding in the Gemini API and Google AI Studio, so 1.5 Pro can now reason across image and audio for videos uploaded in Google AI Studio. And we’re now integrating 1.5 Pro into Google products, including Gemini Advanced and in Workspace apps.Read more about 1.5 Pro in our updated Gemini 1.5 technical report and on the Gemini technology page.Gemini Nano is expanding beyond text-only inputs to include images as well. Starting with Pixel, applications using Gemini Nano with Multimodality will be able to understand the world the way people do — not just through text, but also through sight, sound and spoken language.Read more about Gemini 1.0 Nano on Android.Today, we’re also sharing a series of updates to Gemma, our family of open models built from the same research and technology used to create the Gemini models.We’re announcing Gemma 2, our next generation of open models for responsible AI innovation. Gemma 2 has a new architecture designed for breakthrough performance and efficiency, and will be available in new sizes.The Gemma family is also expanding with PaliGemma, our first vision-language model inspired by PaLI-3. And we’ve upgraded our Responsible Generative AI Toolkit with LLM Comparator for evaluating the quality of model responses.Read more on the Developer blog.As part of Google DeepMind’s mission to build AI responsibly to benefit humanity, we’ve always wanted to develop universal AI agents that can be helpful in everyday life. That’s why today, we’re sharing our progress in building the future of AI assistants with Project Astra (advanced seeing and talking responsive agent).To be truly useful, an agent needs to understand and respond to the complex and dynamic world just like people do — and take in and remember what it sees and hears to understand context and take action. It also needs to be proactive, teachable and personal, so users can talk to it naturally and without lag or delay.While we’ve made incredible progress developing AI systems that can understand multimodal information, getting response time down to something conversational is a difficult engineering challenge. Over the past few years, we’ve been working to improve how our models perceive, reason and converse to make the pace and quality of interaction feel more natural.Building on Gemini, we’ve developed prototype agents that can process information faster by continuously encoding video frames, combining the video and speech input into a timeline of events, and caching this information for efficient recall.By leveraging our leading speech models, we also enhanced how they sound, giving the agents a wider range of intonations. These agents can better understand the context they’re being used in, and respond quickly, in conversation.With technology like this, it’s easy to envision a future where people could have an expert AI assistant by their side, through a phone or glasses. And some of these capabilities are coming to Google products, like the Gemini app and web experience, later this year.We’ve made incredible progress so far with our family of Gemini models, and we’re always striving to advance the state-of-the-art even further. By investing in a relentless production line of innovation, we’re able to explore new ideas at the frontier, while also unlocking the possibility of new and exciting Gemini use cases.Learn more about Gemini and its capabilities. Your information will be used in accordance with Google’s privacy policy.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>      Done. Just one step more.
    
      Check your inbox to confirm your subscription.
    You are already subscribed to our newsletter.
    You can also subscribe with a
    different email address
    
    .
    
  Let’s stay in touch. Get the latest news from Google in your inbox.
          Follow Us
</code></pre></div></div>]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://wvnvwn.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://wvnvwn.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://wvnvwn.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[<h3>External Posts on Your al-folio Blog</h3> <p>If you prefer publishing blog posts on medium.com or other external sources, starting version v0.5.0, <a href="https://github.com/alshedivat/al-folio">al-folio</a> lets you to display your external posts in the blog feed of your website! 🎉🎉</p> <p>Configuring external sources of super simple. After upgrading to v0.5.0, just add the following section to your _config.yml:</p> <pre>external_sources:<br />  - name: medium.com  # name of the source (arbitrary string)<br />    rss_url: <a href="https://medium.com/@al-folio/feed">https://medium.com/@&lt;your-medium-username&gt;/feed</a></pre> <p>The example above adds your medium.com blog post feed as an external source. But you can add arbitrary RSS feeds as sources.</p> <p>Any questions or suggestions? 👉 Start <a href="https://github.com/alshedivat/al-folio/discussions">a discussion on GitHub</a>!</p> <p><img src="https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=b60a1d241a0a" width="1" height="1" alt=""/></p>]]></content><author><name></name></author></entry></feed>