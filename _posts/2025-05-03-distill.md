---
layout: distill
title: "[review] Training Language Models to Follow Instructions with Human Feedback"
description: Fine-tune the language model with PPO algorithm using human feedback.
tags: distill formatting
giscus_comments: true
date: 2025-05-03
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Long Ouyang*
    affiliations:
      name: OpenAI
  - name: Jeff Wu*
    affiliations:
      name: OpenAI
  - name: Xu Jiang*
    affiliations:
      name: OpenAI
  - name: Diogo Almeida*
    affiliations:
      name: OpenAI
  - name: Carroll L. Wainwright*
    affiliations:
      name: OpenAI
  - name: Pamela Mishkin*
    affiliations:
      name: OpenAI
  - name: Chong Zhang
    affiliations:
      name: OpenAI
  - name: Sandhini Agarwal
    affiliations:
      name: OpenAI
  - name: Katarina Slama
    affiliations:
      name: OpenAI
  - name: Alex Ray
    affiliations:
      name: OpenAI
  - name: John Schulman
    affiliations:
      name: OpenAI
  - name: Jacob Hilton
    affiliations:
      name: OpenAI
  - name: Fraser Kelton
    affiliations:
      name: OpenAI
  - name: Luke Miller
    affiliations:
      name: OpenAI
  - name: Maddie Simens
    affiliations:
      name: OpenAI
  - name: Amanda Askell†
    affiliations:
      name: OpenAI
  - name: Peter Welinder
    affiliations:
      name: OpenAI
  - name: Paul Christiano*†
    affiliations:
      name: OpenAI
  - name: Jan Leike*
    affiliations:
      name: OpenAI
  - name: Ryan Lowe*
    affiliations:
      name: OpenAI

bibliography: 2025-05-04-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Abstract
  - name: Motivation
  - name: Data Collection
  - name: Training Methodology
  - name: Evaluation Metric
  - name: Experiments
  - name: Discussion
  - name: Insight

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---

## Abstract
### Main Question
**대규모 언어모델을 사용자의 지시(instruction)에 맞추어 안전하고 유용하게 동작하도록 정렬(alignment)하려면 어떤 학습 절차가 효과적인가?**
- 모델 크기 증가에 따른 성능 증가 (scaling law)가 사용자의 의도를 더 잘 반영하지만은 않는다.
- 사용자의 의도와 목적에 맞는 모델 출력을 위해서는 human feedback이 반영된 데이터셋 기반의 미세 조정이 필요하다.
<p>

### Suggestion
**3단계 RLHF 기반 학습 파이프라인 제안**
- OpenAI API를 통해 제출된 사용자 프롬프트와 labeler들이 작성한 프롬프트를 수집하고, 각 프롬프트에 대해 여러 개의 모델 응답을 생성한 뒤, 생성된 응답에 대해 평가자들이 적절한 응답 순위를 작성
- 수집된 데이터셋을 활용하여 강화학습 기법 통해 GPT-3 모델을 지도학습 방식으로 fine-tuning 진행
<p>

### Contribution
- 100배 더 적은 파라미터를 가진 InstructGPT가 더 적합한 답변을 생성하며, 더 적은 독성 답변 생성과 신뢰가능한 답변 생성을 해내는 것으로 확인됨
- human feedback을 활용한 fine-tuning을 통해 언어 모델을 사용자의 의도에 정렬할 수 있음을 확인

---

## Motivation
Language modeling의 목적식은 next token prediction과 같은 방식의 최적화를 추구한다는 점에서, 사용자의 instruction을 따르는 최적화 목적식과는 다르기 때문 (The language modeling objective used for many recent large LMs is different from the objective ‘follow the user’s instructions helpfully and safely’. We say that the language modeling objective is misaligned.)
- 강화학습의 알고리즘을 활용하여, human preference를 reward signal로 사용하여 모델을 fine-tuning하여 user preference 혹은 user intent를 최적화 목적식으로 설정

---

## Data Collection
어떤 데이터를 학습에 사용했는가? <p>
### Prompts
API를 통해 실제 사용자가 제출한 입력 prompt와 labeler-written instruction-like prompt를 사용
- 실제 사용자 prompt
    - Playground  인터페이스틑 롱해 InstructGPT 초기 모델을 사용한 사용자들이 제출한 프롬프트만 수집 (실제 API 사용자들의 데이터는 사용하지 않음)
    - 프롬프트 전처리
        - 유사 문장의 반복을 막기 위해 공통적인 prefix 부분을 제거
        - 유저당 최대 200개의 prompt만을 선택
        - train/valid/test split을 진행
        - 개인정보 필터링: PII 삭제 <p>
- labeler-written instruction-like prompt
    - 초기 모델 학습에 필요한 instruction-following 데이터 확보를 위해 labeler-written prompt를 사용
    - bootstrap 역할 수행
    - Plain, Few-shot, User-based 종류로 prompt를 작성 <p>
<d-figure>
  <img src="/assets/img/2025-05-03-distill-1.png" alt="선호도 증가" width="500">
  <figcaption>Figure 1: API Prompt Composition</figcaption>
</d-figure><p>

### 데이터셋 구성
- SFT 데이터셋: Prompt + Labeler-written demonstrations
- RM 데이터셋: Prompt + 모델 출력에 대한 pair-wise ranking
- PPO 데이터셋: Prompt와 reward 값 (사실상 prompt만 사용하고 prompt에 대한 답변의 reward만 사용)

---

## Training Methodology
어떤 학습 방법을 사용했는가? <p>

1. **Supervised Fine-tuning**
    - 40명의 라벨러들이 사용자 프롬프트에 대한 이상적인 응답을 작성
    - 프롬프트-응답 쌍을 활용하여 모델을 fine-tuning <p>
2. **Reward Model 학습**
    - 어떤 응답이 더 우수한지를 예측하는 reward model을 훈련
    - Reward Model
        - SFT 모델에서 **final unembedding layer**를 제거하고, MLP를 붙여 scalar output을 반환하도록 구성
            - transformer layer를 통과한 hidden state가 MLP를 통과하여 scalar 값을 반환하는 구조로 reward model을 구성
    - 학습 데이터
        - 같은 prompt에 대해 생성된 서로 다른 두 응답에 대해, 어느 쪽이 더 나은지를 사람이 라벨링한 데이터를 학습
            - 이때, 하나의 prompt에 대해 K = 4~9개의 응답을 수집하고, 주어진 K개의 응답에서 서로 다른 2개의 응답 비교쌍을 만들어서 활용
            - 하나의 prompt에 대한 모든 비교 쌍을 하나의 batch element로 학습하여 correlated sample 문제 해결 (overfitting의 원인)
    - 손실 함수
        - $\text{loss }(\theta) = -{1\over \binom{k}{2}}E_{(x, y_w, y_l) \sim D}[\log (\sigma(r_\theta(x, y_w) - r_\theta(x, y_l)))]$ <p>
3. **RLHF with PPO**
    - PPO 알고리즘을 적용하여 모델을 강화학습 방식으로 추가 fine-tuning
    - Bandit Environment
        - 개념: 단 한 번의 행동만으로 즉각적인 보상을 받고 episode가 종료되는 구조
        - 주어진 prompt에 대한 response의 step-wise 최적화를 위해 bandit environment를 가정
    - Value Model
        - 학습된 RM model을 초기값으로 하여 모델과 함께 업데이트 됨
    - 손실 함수 (Policy Loss)
        - $\text{objective }(\phi) = E_{(x, y) \sim D_{\pi_\phi^{RL}}}[r_\theta(x, y) - \beta \log({\pi_\phi^{RL}(y | x) \over \pi^{\text{SFT}}(y|x)})] + \gamma E_{x \sim D_{\text{pretrain}}}[\log(\pi_\phi^{RL}(x))]$
            - KL-divergence 항
            - if $\gamma \neq 0$, PPO-ptx model

---

## Evaluation Metric
성능 평가를 어떻게 진행하였는가? <p>

### Baseline Model
- PPO 모델을 GPT-3 & GPT-3-prompted 모델 및 SFT 모델과 비교 <p>

### Alignment의 개념
- 사용자의 의도에 부합되기 훈련된 모델이 행동하는 것
    - helpful: they should help the user solve their task
        - 모델은 instruction에 부합한 output을 생성해야
    - honest: they shouldn’t fabricate information or mislead the user
        - hallucination을 유발하는 것인지를 평가하고
        - TruthfulQA dataset을 활용하여 평가
    - harmless: they shouldn’t cause physical, psychological, or social harm to people or the environment

---

## Experiments
1. **실사용 API에 대한 결과 - 175B 모델**
- **API Prompt 결과**
    - GPT < GPT-prompted < SFT < PPO < PPO-ptx 순으로 선호도가 증가하는 경향을 보임.
<d-figure>
  <img src="/assets/img/2025-05-03-distill-2.png" alt="선호도 증가">
  <figcaption>Figure 2: API Prompt 결과 선호도 비교</figcaption>
</d-figure>
    - 세부 기준에 따라 선호도를 평가한 결과.
<d-figure>
  <img src="/assets/img/2025-05-03-distill-3.png" alt="선호도 증가">
  <figcaption>Figure 3: 세부 기준 선호도 평가 결과</figcaption>
</d-figure>

- **InstructGPT의 일반화 성능**
    - Reward model의 학습 과정 참여 여부를 기준으로 worker들을 구분하여, InstructGPT가 reward model에 과도하게 optimize 되었는지를 분석하고, 훈련 과정에 사용된 InstructGPT prompt와 그렇지 않은 GPT-3 prompt를 사용하여 선호로들 평가함으로써 InstructGPT의 일반화 성능을 평가.
<d-figure>
  <img src="/assets/img/2025-05-03-distill-4.png" alt="선호도 증가">
  <figcaption>Figure 4: InstructGPT 일반화 성능 평가 결과</figcaption>
</d-figure>

- **공개된 NLP dataset-based FT 모델과 비교**
    - FLAN과 T0 dataset으로 FT 모델과 비교한 결과, SFT 모델보다는 좋지 못함 ⇒ 이 데이터셋은 선호도 반영을 위해서 언어 모델이 사용하는 방식을 반영하지 않음을 보임.
<d-figure>
  <img src="/assets/img/2025-05-03-distill-5.png" alt="선호도 증가">
  <figcaption>Figure 5: 공개 NLP 데이터셋 기반 FT 모델 비교 결과</figcaption>
</d-figure>

2. **공개 NLP Benchmark에 대한 결과**
- **Truthfulness (Honesty)**
<d-figure>
  <img src="/assets/img/2025-05-03-distill-6.png" alt="선호도 증가">
  <figcaption>Figure 6: Truthfulness (Honesty) 평가 결과</figcaption>
</d-figure>
    - TruthfulQA dataset 평가 결과로, 각각 1.3/6/175B 모델
    - 회색은 informative
    - human evaluation을 진행한 것<p><p>

- **Toxicity**
<d-figure>
  <img src="/assets/img/2025-05-03-distill-7.png" alt="선호도 증가">
  <figcaption>Figure 7: Toxicity 평가 결과</figcaption>
</d-figure>
    - RealToxicityPrompts dataset 평가 결과
    - human evaluation과 PerspectiveAPI 결과를 동시에 활용
    - Respectful: safe and respectful output을 반환하도록 하는 prompt를 추가한 것<p><p>

- **Alignment Tax**
    - **Alignment tax**: API 분포로 PPO 미세조정을 한 모델은 일반 NLP 벤치마크 성능이 저해되는 문제가 발생
    - Alignment tax 문제를 극복하기 위해 PPO-ptx를 활용하여, 사전학습 gradient를 일정 비율로 혼합하여 모델을 업데이트하는 방법을 사용<p><p>

3. **정성 평가**
- non-english language 혹은 코드에 대한 질문에 대해서도 답변을 수행.

---

## Discussion
의미 있다고 생각되는 Discussion 내용만 추림.
- **The cost of increasing model alignment is modest relative to pretraining!**
- **Alignment techniques are need to reduce alignment-tax!**

---

## Insight
1. **언어모델로 하여금 instruction을 따르는 답변을 생성하도록 훈련하고 guidance 하는 것과, 선호를 표현하고 가치를 따르는 것은 다른 개념이라는 점에서, 이를 명확하게 구분하는 것이 필요함** <p>
2. **Preference optimization 과정에서 instruction-following과 prefenrece-reveal 혹은 following을 명확히 구분하는 것이 필요하며, 이를 위해서는 개념을 명확히 분리하는 것이 필요함**